{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMamjBniOyePZ+k50yDXSbH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"WJEjEMz8ndi6","executionInfo":{"status":"ok","timestamp":1706094460946,"user_tz":-330,"elapsed":501,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","from matplotlib import ticker, cm\n","from numpy.linalg import inv\n","import time"]},{"cell_type":"markdown","source":["#Exercise 3 (20 marks)\n","\n"," Consider the functions $f(x) = f(x_1, x_2) = x_1^2 + x_2^2 + 9$,  $ g(x) = g(x_1, x_2,x_3, ...., x_n) =\\sum_{i}^n\\frac{1}{P(i)}(x_i - i^2)^2  $\n",", Where $P(y) (y ∈ R)$ is  a  periodic function with the period of 4 and $P(7) = \\frac{1}\n","{4} , P(77) =\n","\\frac{1}\n","{16} , P(222) = \\frac{1}\n","{256}, P(4444) =\\frac{1}\n","{64} .$\n"],"metadata":{"id":"A74txaF7ntNN"}},{"cell_type":"markdown","source":["**1. What is the minimizer and minimum function value of f(x) and g(x) ? Are both the function convex ? Explain.**"],"metadata":{"id":"AzQk1UJW_QOU"}},{"cell_type":"markdown","source":["For f(x) it is seen that the minimum function value will be 9 and the minimum  value of g(x) will be 0"],"metadata":{"id":"wAe4PzPSP1wj"}},{"cell_type":"code","source":[],"metadata":{"id":"XVZ7YFmavder"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Implement Gradient Descent with the exact line search for f(x) and also implement Newton’s Method (From\n","LAB-01) for f(x). Note down the time taken, number of iterations required for convergence, record the final\n","minimizer, final objective function value for both the implementations. Provide observations on the results, and\n","comment on the minimizers and objective function values so obtained. Plot the level sets of the function f(x)\n","and also plot the trajectory of the optimization on the same plot for both the implementations and report your\n","observations. (Take $τ = 10^{−15}$, $x_0 = (1000, −1000).)$\n"],"metadata":{"id":"cT8tA7JSve11"}},{"cell_type":"code","source":["def f(x):\n","  return x[0]**2 +x[1]**2 + 9\n","\n","def grad_f(x):\n","  return np.array([2*x[0] , 2*x[1]])\n","\n","def grad_descent(f,grad_f,x0,tolerance,step_size):\n","  x = x0\n","  k = 0\n","  while np.linalg.norm(grad_f(x)) >tolerance:\n","    x = x - step_size * grad_f(x)\n","    k = k+1\n","  return x,k\n","\n","# FOR NEWTON'S METHOD\n","\n","def hessian_f(x):\n","   return np.array([[2,0],[0,2]])\n","\n","\n","\n","\n","def newtons_method(f, grad_f, hessian_f, x0, tolerance):\n","    x = x0\n","    while np.linalg.norm(grad_f(x)) > tolerance:\n","        hessian_inv = inv(hessian_f(x))\n","        x = x - hessian_inv @ grad_f(x)\n","    return x\n"],"metadata":{"id":"fwT1SkvsvtTC","executionInfo":{"status":"ok","timestamp":1706093198164,"user_tz":-330,"elapsed":6,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["x0 = np.array([1000,-1000])\n","tolerance = 10**-15\n","step_size = 0.5\n","\n","minimizer,iteration = grad_descent(f,grad_f,x0,tolerance,step_size)\n","\n"],"metadata":{"id":"0V0znNRMwLFB","executionInfo":{"status":"ok","timestamp":1706093222713,"user_tz":-330,"elapsed":610,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(minimizer,iteration)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3p7aF6nhFVrU","executionInfo":{"status":"ok","timestamp":1706093279628,"user_tz":-330,"elapsed":6,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}},"outputId":"96e68c9d-d9c0-4dc0-fb4f-40f6e03f3978"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0.] 1\n"]}]},{"cell_type":"code","source":["newtons_method(f, grad_f, hessian_f, x0, tolerance)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tK18IGqG0ick","executionInfo":{"status":"ok","timestamp":1706093297927,"user_tz":-330,"elapsed":538,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}},"outputId":"34309662-9875-4196-c23c-cd103753d543"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0.])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["iteration"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmcxA_-70kEn","executionInfo":{"status":"ok","timestamp":1706038550731,"user_tz":-330,"elapsed":5,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}},"outputId":"e3da510e-a6b3-467b-9d18-51090fa9fc84"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["3. For n ∈ {2, 20, 200, 2000, 5000, 10000, 15000, 20000, 30000, 50000, 100000, 200000} Implement Gradient Descent\n","with the exact line search for g(x) and also implement Newton’s Method (From LAB-01) for g(x). Note down\n","the time taken, number of iterations required for convergence, record the final minimizer, final objective function\n","value for both the implementations. Provide observations on the results, and comment on the minimizers and\n","objective function values so obtained. Only for n = 2 plot the level sets of the function g(x) and also plot\n","the trajectory of the optimization on the same plot for both the implementations and report your observations.\n","(Take τ =$ 10^{-15}$, $x_0 = (1, 2, 3, ....., n)$.)"],"metadata":{"id":"QZlmAz9L9jXP"}},{"cell_type":"code","source":["def P(y):\n","    periods = [4, 1/4, 1/16, 1/256, 1/64]\n","    return periods[y % 4]\n","\n","def g(x):\n","  return sum([(x_i - i**2)**2 / P(i) for i, x_i in enumerate(x, start=1)])\n","\n","def grad_g(x):\n","    return np.array([2 * (x_i - i**2) / P(i) for i, x_i in enumerate(x, start=1)])\n","\n"],"metadata":{"id":"fAFPmkei0lqa","executionInfo":{"status":"ok","timestamp":1706093854888,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","tolerance = 10**-15\n","step_size = 0.5\n","n_values = [2, 20, 200, 2000, 5000, 10000, 15000, 20000, 30000, 50000, 100000, 200000]\n","minimizer,iteration = grad_descent(g,grad_g,x0,tolerance,step_size)\n","for n in n_values:\n","    x0 = np.arange(1, n + 1)  # Initial point in n-dimensional space\n","    minimizer,  num_iterations = grad_descent(g, grad_g, x0, tolerance,step_size)\n","    print(f\"\\nGradient Descent with Exact Line Search for g(x) with n = {n}:\")\n","    print(\"Final Minimizer:\", minimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iq7S3OXbAJL1","executionInfo":{"status":"ok","timestamp":1706094005552,"user_tz":-330,"elapsed":148265,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}},"outputId":"1e50ed06-c486-4343-f3e4-6425ffd17270"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-911b01b9709d>:9: RuntimeWarning: overflow encountered in double_scalars\n","  return np.array([2 * (x_i - i**2) / P(i) for i, x_i in enumerate(x, start=1)])\n","<ipython-input-1-0640510628b8>:11: RuntimeWarning: invalid value encountered in subtract\n","  x = x - step_size * grad_f(x)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Gradient Descent with Exact Line Search for g(x) with n = 2:\n","Final Minimizer: [ 1. nan]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 20:\n","Final Minimizer: [ 1.00000000e+000 -6.92964768e+150             -inf  1.60000000e+001\n"," -2.35803692e+062 -1.03944715e+152             -inf  6.40000000e+001\n"," -8.48893290e+062 -3.11834146e+152             -inf  1.44000000e+002\n"," -1.83926879e+063 -6.30597939e+152              nan  2.56000000e+002\n"," -3.20693021e+063 -1.06023610e+153              nan  4.00000000e+002]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 200:\n","Final Minimizer: [ 1.00000000e+000 -6.92964768e+150             -inf  1.60000000e+001\n"," -2.35803692e+062 -1.03944715e+152             -inf  6.40000000e+001\n"," -8.48893290e+062 -3.11834146e+152             -inf  1.44000000e+002\n"," -1.83926879e+063 -6.30597939e+152              nan  2.56000000e+002\n"," -3.20693021e+063 -1.06023610e+153              nan  4.00000000e+002\n"," -4.95187752e+063 -1.60074861e+153              nan  5.76000000e+002\n"," -7.07411075e+063 -2.25213550e+153              nan  7.84000000e+002\n"," -9.57362988e+063 -3.01439674e+153              nan  1.02400000e+003\n"," -1.24504349e+064 -3.88753235e+153              nan  1.29600000e+003\n"," -1.57045259e+064 -4.87154232e+153              nan  1.60000000e+003\n"," -1.93359027e+064 -5.96642666e+153              nan  1.93600000e+003\n"," -2.33445655e+064 -7.17218535e+153              nan  2.30400000e+003\n"," -2.77305141e+064 -8.48881841e+153              nan  2.70400000e+003\n"," -3.24937487e+064 -9.91632583e+153              nan  3.13600000e+003\n"," -3.76342692e+064 -1.14547076e+154              nan  3.60000000e+003\n"," -4.31520756e+064 -1.31039638e+154              nan  4.09600000e+003\n"," -4.90471678e+064 -1.48640943e+154              nan  4.62400000e+003\n"," -5.53195460e+064 -1.67350992e+154              nan  5.18400000e+003\n"," -6.19692101e+064 -1.87169784e+154              nan  5.77600000e+003\n"," -6.89961601e+064 -2.08097320e+154              nan  6.40000000e+003\n"," -7.64003961e+064 -2.30133600e+154              nan  7.05600000e+003\n"," -8.41819179e+064 -2.53278623e+154              nan  7.74400000e+003\n"," -9.23407256e+064 -2.77532390e+154              nan  8.46400000e+003\n"," -1.00876819e+065 -3.02894900e+154              nan  9.21600000e+003\n"," -1.09790199e+065 -3.29366154e+154              nan  1.00000000e+004\n"," -1.19080864e+065 -3.56946152e+154              nan  1.08160000e+004\n"," -1.28748816e+065 -3.85634894e+154              nan  1.16640000e+004\n"," -1.38794053e+065 -4.15432379e+154              nan  1.25440000e+004\n"," -1.49216576e+065 -4.46338607e+154              nan  1.34560000e+004\n"," -1.60016385e+065 -4.78353580e+154              nan  1.44000000e+004\n"," -1.71193480e+065 -5.11477295e+154              nan  1.53760000e+004\n"," -1.82747861e+065 -5.45709755e+154              nan  1.63840000e+004\n"," -1.94679528e+065 -5.81050958e+154              nan  1.74240000e+004\n"," -2.06988480e+065 -6.17500905e+154              nan  1.84960000e+004\n"," -2.19674719e+065 -6.55059595e+154              nan  1.96000000e+004\n"," -2.32738244e+065 -6.93727030e+154              nan  2.07360000e+004\n"," -2.46179054e+065 -7.33503207e+154              nan  2.19040000e+004\n"," -2.59997150e+065 -7.74388129e+154              nan  2.31040000e+004\n"," -2.74192533e+065 -8.16381794e+154              nan  2.43360000e+004\n"," -2.88765201e+065 -8.59484202e+154              nan  2.56000000e+004\n"," -3.03715155e+065 -9.03695354e+154              nan  2.68960000e+004\n"," -3.19042395e+065 -9.49015250e+154              nan  2.82240000e+004\n"," -3.34746921e+065 -9.95443890e+154              nan  2.95840000e+004\n"," -3.50828732e+065 -1.04298127e+155              nan  3.09760000e+004\n"," -3.67287830e+065 -1.09162740e+155              nan  3.24000000e+004\n"," -3.84124214e+065 -1.14138227e+155              nan  3.38560000e+004\n"," -4.01337883e+065 -1.19224588e+155              nan  3.53440000e+004\n"," -4.18928838e+065 -1.24421824e+155              nan  3.68640000e+004\n"," -4.36897080e+065 -1.29729934e+155              nan  3.84160000e+004\n"," -4.55242607e+065 -1.35148919e+155              nan  4.00000000e+004]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 2000:\n","Final Minimizer: [1.00000000e+000 4.61976512e+149 2.56303281e+306 ... 9.21644528e+155\n","             nan 4.00000000e+006]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 5000:\n","Final Minimizer: [ 1.00000000e+000 -3.07984341e+148 -1.00511091e+304 ... -3.84595539e+155\n","              nan  2.50000000e+007]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 10000:\n","Final Minimizer: [ 1.00000000e+000 -3.07984341e+148 -1.00511091e+304 ... -1.53915184e+156\n","              nan  1.00000000e+008]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 15000:\n","Final Minimizer: [ 1.00000000e+000 -3.07984341e+148 -1.00511091e+304 ... -3.46366899e+156\n","              nan  2.25000000e+008]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 20000:\n","Final Minimizer: [ 1.00000000e+000 -3.07984341e+148 -1.00511091e+304 ... -6.15814700e+156\n","              nan  4.00000000e+008]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 30000:\n","Final Minimizer: [ 1.00000000e+000 -3.07984341e+148 -1.00511091e+304 ... -1.38569856e+157\n","              nan  9.00000000e+008]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 50000:\n","Final Minimizer: [ 1.00000000e+000 -3.07984341e+148 -1.00511091e+304 ... -3.84941930e+157\n","              nan  2.50000000e+009]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 100000:\n","Final Minimizer: [1.00000000e+000 2.05322894e+147 3.94161139e+301 ... 1.02656314e+157\n","             nan 1.00000000e+010]\n","\n","Gradient Descent with Exact Line Search for g(x) with n = 200000:\n","Final Minimizer: [1.00000000e+000 2.05322894e+147 3.94161139e+301 ... 4.10635523e+157\n","             nan 4.00000000e+010]\n"]}]},{"cell_type":"code","source":["def hessian_g(x):\n","    n = len(x)\n","    hess = np.zeros((n, n))\n","    for i in range(n):\n","        for j in range(n):\n","            hess[i, j] = 2 * (1 - 2 * i**2 * (x[j] - j**2)) / P(i)**2 if i == j else -4 * i * (x[i] - i**2) / P(i)**2\n","    return hess\n","\n","def newtons_method_g(g, grad_g, hessian_g, x0, tau):\n","    x = x0\n","    k = 0\n","    grad = grad_g(x)\n","\n","\n","    while np.linalg.norm(grad) > tau:\n","        hess_inv = np.linalg.inv(hessian_g(x))\n","        x = x - hess_inv.dot(grad_g(x))\n","        grad = grad_g(x)\n","        k += 1\n","\n","\n","\n","    return x, g(x), k\n","\n","x0 = np.array([1000, -1000])  # Initial point in n-dimensional space\n","tau = 1e-15\n","minimizer, min_obj_val, num_iterations = newtons_method_g(g, grad_g, hessian_g, x0, tau)\n","\n","print(\"\\nNewton's Method for g(x):\")\n","print(\"Final Minimizer:\", minimizer)\n","print(\"Final Objective Function Value:\", min_obj_val)\n","print(\"Number of Iterations:\", num_iterations)\n"],"metadata":{"id":"pvt444FdDbMs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706094107413,"user_tz":-330,"elapsed":667,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}},"outputId":"82ab2578-0e27-47f9-bdb4-d884d40de64c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Newton's Method for g(x):\n","Final Minimizer: [nan nan]\n","Final Objective Function Value: nan\n","Number of Iterations: 170\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-911b01b9709d>:9: RuntimeWarning: overflow encountered in double_scalars\n","  return np.array([2 * (x_i - i**2) / P(i) for i, x_i in enumerate(x, start=1)])\n","<ipython-input-11-fef196140aab>:6: RuntimeWarning: overflow encountered in double_scalars\n","  hess[i, j] = 2 * (1 - 2 * i**2 * (x[j] - j**2)) / P(i)**2 if i == j else -4 * i * (x[i] - i**2) / P(i)**2\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZgJaz_OPHJV4"},"execution_count":null,"outputs":[]}]}