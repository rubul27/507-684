{"cells":[{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707240586961,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"},"user_tz":-330},"id":"m8jQEKOjl8XG"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from numpy import ma\n","from matplotlib import ticker, cm\n","from math import sqrt, pi\n","from numpy import exp, cos, sin\n","from numpy.linalg import norm\n","import time"]},{"cell_type":"markdown","metadata":{"id":"u4MZo4IH0Qlb"},"source":["#Algorithm 3 BFGS Algorithm\n","\n","\n","Require: Starting point $x_0$, Stopping tolerance τ\n","\n","1: Initialize k = 0, $B_0$ =??           ▷ Initialize Hessian approximation\n","\n","2: while $∥∇f(x_k)∥_2 > τ$ do\n","\n","3: Compute descent direction: $p_k = −B_k∇f(x_k)$\n","\n","4: Choose step size: $α_k = arg min_{α≥0} f(x_k + αp_k)$\n","\n","5: Update iterate: $x_{k+1} = x_k + α_kp_k$\n","\n","6: Compute new gradient: $s_k = x_{k+1} − x_k, y_k = ∇f(x_{k+1}) − ∇f(x_k)$\n","\n","7: Update Hessian approximation: B_{k+1} =?? ▷ Replace ?? by the update mentioned\n","in BFGS Theory\n","\n","8: k = k + 1\n","\n","9: Output: $x_k$\n"]},{"cell_type":"markdown","metadata":{"id":"3wgKMNha8CC_"},"source":["**$f(x) = f(x_1, x_2, x_3, ...., x_n) = \\sum_i^{n-1} [4(x_i^2 − x_i+1)^2 + (x_i − 1)^2]$**\n","\n","**$g(x) = g(x_1, x_2, x_3, ...., x_n) = \\sum_i^n[(x_1 − x_i^2)^2 + (x_i − 1)^2]$**"]},{"cell_type":"markdown","metadata":{"id":"v6lrjtdD9Lgx"},"source":["**1. What is the minimizer and minimum function value of f(x) and g(x) ? Are both the function convex ? What\n","is a suitable initial choice of B (denoted by B0, i.e. Replacement of first ?? in the Algorithm 3)? Justify\n","with proper reasons.**"]},{"cell_type":"markdown","metadata":{"id":"VhZlMZH-9TsY"},"source":["To find the minimizer and minimum function value of $ f(x) $ and $ g(x) $, we need to solve for the critical points of these functions and then check whether they correspond to minima.\n","\n","### Minimizer and Minimum Function Value:\n","\n","#### Function $ f(x) $:\n","The minimizer and minimum function value of $ f(x) $ can be found by setting its gradient to zero and solving for $ x $. However, due to the complexity of the function and the presence of multiple variables, finding an analytical solution may be challenging. Numerical optimization techniques like gradient descent or Newton's method can be used to find the minimizer and minimum function value.\n","\n","#### Function $ g(x) $:\n","Similarly, for $( g(x) $), we need to find the critical points by setting its gradient to zero and solving for $( x $). Again, numerical optimization techniques may be needed due to the complexity of the function.\n","\n","### Convexity:\n","\n","#### Function $ f(x) $:\n","To determine if  f(x) is convex, we need to check if its Hessian matrix is positive semidefinite for all  x . Since the Hessian involves second-order derivatives of $ f(x) $, its computation and analysis can be complex and may require numerical methods or specialized software.\n","\n","#### Function $ g(x) $:\n","Similar to $ f(x) $, we need to examine the Hessian of $ g(x) $ to determine its convexity.\n","\n","### Initial Choice of $ B_0$  in Algorithm 3:\n","\n","The initial choice of $B_0 $ in the BFGS algorithm is crucial for the convergence and efficiency of the algorithm. A suitable initial choice for $ B_0 $ is often the identity matrix or a positive definite matrix that is a good approximation of the Hessian matrix at the starting point. This choice helps ensure that the update formula in the BFGS algorithm generates a positive definite approximation of the Hessian at each iteration, maintaining the positive definiteness of the approximation throughout the optimization process.\n","\n","In summary, finding the minimizer and minimum function value of $ f(x) $ and $( g(x) $ involves solving for critical points, which can be complex due to the functions' multivariate nature. Determining the convexity of the functions requires analyzing their Hessian matrices. A suitable initial choice for $ B_0 $ in the BFGS algorithm is typically the identity matrix or a positive definite approximation of the Hessian at the starting point to ensure convergence and stability."]},{"cell_type":"markdown","metadata":{"id":"McFeT1-w3ko_"},"source":[]},{"cell_type":"markdown","metadata":{"id":"babBdLiQ-DUc"},"source":["**2. Implement Algorithm 3 for solving $min_{x∈R^n} f(x)$, Use backtracking line search with $α_0 = 0.9, ρ = 0.5$, γ = 0.5.\n","Take the starting point to be $x_0 = (0, 0, ..., 0)$. Take n ∈ {1000, 2500, 5000, 7500, 10000}, find minimizer of the\n","objective function in each case and compute the time taken by the BFGS method with backtracking line search.\n","Tabulate the time taken by BFGS method for each n.**"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":513,"status":"ok","timestamp":1707240264801,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"},"user_tz":-330},"id":"602C_mXpGAY7"},"outputs":[],"source":["def fx(xk):\n","  length = len(xk)\n","  sum = 0\n","  for i in range(length-1):\n","    sum+= 4*(xk[i]**2 - xk[i+1])**2 + (xk[i]-1)**2\n","  return sum\n","\n","def gradient_fx(xk):\n","  n = len(xk)\n","  grad = []\n","  grad.append( 16*xk[0]*(xk[0]**2 - xk[1]) + 2*(xk[0]-1) )\n","  for i in range(1, n-1):\n","    grad.append(  -8*(xk[i-1]**2 - xk[i]) + 16*xk[i]*(xk[i]**2 - xk[i+1]) + 2*(xk[i]-1) )\n","  grad.append(-8*(xk[n-2]**2 - xk[n-1]))\n","  return np.array(grad)"]},{"cell_type":"code","source":[],"metadata":{"id":"XwpqTr2Acwnf","executionInfo":{"status":"ok","timestamp":1707240296210,"user_tz":-330,"elapsed":483,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"8ho4QR5xcwLG"}},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707231455476,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"},"user_tz":-330},"id":"FTXstIROGAKF"},"outputs":[],"source":["def get_alpha_bfgs(xk, alpha0, rho, gamma, Bk):\n","  alpha = alpha0\n","  pk = -gradient_fx(xk)\n","  while fx(xk + alpha*Bk@pk) > (fx(xk) + gamma*alpha*gradient_fx(xk)@Bk@pk):\n","    alpha = rho*alpha\n","  return alpha\n","\n","\n","def bfgs(x0, tau, alpha0, rho, gamma, max_iter=500):\n","  start_time = time.time()\n","  xk = np.copy(x0)\n","  n = len(x0)\n","  Bk = np.eye(n)\n","  count = 0\n","  pk = gradient_fx(xk)\n","  xks = []\n","  xks.append(xk)\n","  while (norm(pk)>tau):\n","    if count > max_iter:\n","      break\n","\n","    alpha = get_alpha_bfgs(xk, alpha0, rho, gamma, Bk)\n","    xnext = xk - alpha*(Bk@pk)\n","\n","    # print(\"new xk: \", xk[0:5])\n","    ## new Bk+1 computation\n","    sk = xnext - xk\n","    yk = gradient_fx(xnext) - gradient_fx(xk)\n","    # BFGS update formula\n","    Bk = np.dot((np.eye(len(xk)) - np.outer(sk, yk) / np.dot(yk, sk)), np.dot(Bk, (np.eye(len(xk)) - np.outer(yk, sk) / np.dot(yk, sk)))) + np.outer(sk, sk) / np.dot(yk, sk)\n","\n","    xk = xnext\n","    pk = gradient_fx(xk)\n","    # print(\"grad is: \", pk[0:5])\n","    # print(\"grad norm is: \", norm(pk))\n","    xks.append(xk)\n","    count += 1\n","\n","  end_time = time.time()\n","  time_elapsed = end_time - start_time\n","  return count, xk, fx(xk), xks, time_elapsed"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPHTN3VZGixq","executionInfo":{"status":"ok","timestamp":1707237828249,"user_tz":-330,"elapsed":6342153,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}},"outputId":"6d6b0518-b568-486a-8219-59827cbf05dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["for n = 1000\n","total iterations: 17 | minimizer: [0.99772206 0.99874487 0.99972807 0.99895936 1.00007376] | minimum: 0.1514899862210296 | time taken: 4.983297109603882\n","----------------------------------------------------------------\n","for n = 2500\n","total iterations: 19 | minimizer: [0.99840685 0.99967232 1.00024808 0.99757509 1.00077045] | minimum: 0.1276955357858959 | time taken: 46.2685272693634\n","----------------------------------------------------------------\n","for n = 5000\n","total iterations: 36 | minimizer: [0.99761426 0.99548031 0.990543   0.9904915  0.99011987] | minimum: 0.1585618341870785 | time taken: 590.1595966815948\n","----------------------------------------------------------------\n","for n = 7500\n","total iterations: 35 | minimizer: [1.00581333 1.00531036 1.00092941 1.00088032 1.00268388] | minimum: 0.19255899400483575 | time taken: 1834.519108057022\n","----------------------------------------------------------------\n","for n = 10000\n","total iterations: 32 | minimizer: [0.99828304 0.99509938 0.9950328  0.99516837 0.99865665] | minimum: 0.259670310747407 | time taken: 3865.7793600559235\n","----------------------------------------------------------------\n"]}],"source":["alpha0 = 0.9\n","rho = 0.5\n","gamma = 0.5\n","ns = [1000, 2500, 5000, 7500, 10000]\n","tau = 2\n","for n in ns:\n","  x0 = [0 for i in range(n)]\n","  count, minimizer, minimum, xks, time_elapsed=bfgs(x0,tau, alpha0, rho, gamma)\n","  print(f\"for n = {n}\")\n","  print(f\"total iterations: {count} | minimizer: {minimizer[0:5]} | minimum: {minimum} | time taken: {time_elapsed}\")\n","  print(\"----------------------------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"_bGuxCk-C_zW"},"source":["**3. Implement Algorithm 3 for solving $min_{x∈R^n} g(x)$, Use backtracking line search with α0 = 0.9, ρ = 0.5, γ = 0.5.\n","Take the starting point to be x0 = (0, 0, ..., 0). Take n ∈ {1000, 2500, 5000, 7500, 10000}, find minimizer of the\n","objective function in each case and compute the time taken by the BFGS method with backtracking line search.\n","Tabulate the time taken by BFGS method for each n.**\n"]},{"cell_type":"markdown","metadata":{"id":"5UeyhAKUNMkG"},"source":["**$f(x) = f(x_1, x_2, x_3, ...., x_n) = \\sum_i^{n-1} [4(x_i^2 − x_i+1)^2 + (x_i − 1)^2]$**\n","\n","**$g(x) = g(x_1, x_2, x_3, ...., x_n) = \\sum_i^n[(x_1 − x_i^2)^2 + (x_i − 1)^2]$**"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"GhpfeyG_P0X5","executionInfo":{"status":"ok","timestamp":1707240685268,"user_tz":-330,"elapsed":425,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}}},"outputs":[],"source":["def gx(xk):\n","  length = len(xk)\n","  sum = 0\n","  for i in range(length-1):\n","    sum+= (xk[0] - xk[i]**2)**2 + (xk[i]-1)**2\n","  return sum\n","\n","def gradient_gx(xk):\n","  n = len(xk)\n","  grad = []\n","  grad.append((2*(xk[0] - xk[0]**2) )*(1-2*xk[0]) + 2*(xk[0]-1))\n","\n","  for i in range(1, n):\n","    grad.append( -4*xk[i]*(xk[0] - xk[i]**2)+2*(xk[i]-1) )\n","\n","  return np.array(grad)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"oBMaVWbHPiQd","executionInfo":{"status":"ok","timestamp":1707240687948,"user_tz":-330,"elapsed":712,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}}},"outputs":[],"source":["def get_alpha_bfgs(xk, alpha0, rho, gamma, Bk):\n","  alpha = alpha0\n","  pk = -gradient_gx(xk)\n","  while gx(xk + alpha*Bk@pk) > (gx(xk) + gamma*alpha*gradient_gx(xk)@Bk@pk):\n","    alpha = rho*alpha\n","  return alpha\n","\n","\n","def bfgs(x0, tau, alpha0, rho, gamma, max_iter=500):\n","  start_time = time.time()\n","  xk = np.copy(x0)\n","  n = len(x0)\n","  Bk = np.eye(n)\n","  count = 0\n","  pk = gradient_gx(xk)\n","  xks = []\n","  xks.append(xk)\n","  while (norm(pk)>tau):\n","    if count > max_iter:\n","      break\n","\n","    alpha = get_alpha_bfgs(xk, alpha0, rho, gamma, Bk)\n","    xnext = xk - alpha*(Bk@pk)\n","\n","\n","    sk = xnext - xk\n","    yk = gradient_gx(xnext) - gradient_gx(xk)\n","\n","    Bk = np.dot((np.eye(len(xk)) - np.outer(sk, yk) / np.dot(yk, sk)), np.dot(Bk, (np.eye(len(xk)) - np.outer(yk, sk) / np.dot(yk, sk)))) + np.outer(sk, sk) / np.dot(yk, sk)\n","\n","    xk = xnext\n","    pk = gradient_gx(xk)\n","\n","    xks.append(xk)\n","    count += 1\n","\n","  end_time = time.time()\n","  time_elapsed = end_time - start_time\n","  return count, xk, fx(xk), xks, time_elapsed"]},{"cell_type":"code","source":["alpha0 = 0.9\n","rho = 0.5\n","gamma = 0.5\n","ns = [1000, 2500, 5000, 7500, 10000]\n","tau = 2\n","for n in ns:\n","  x0 = [0 for i in range(n)]\n","  count, minimizer, minimum, xks, time_elapsed = bfgs(x0,tau, alpha0, rho, gamma)\n","  print(f\"for n = {n}\")\n","  print(f\"total iterations: {count} | minimizer: {minimizer[0:5]} | minimum: {minimum} | time taken: {time_elapsed}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSk6JI9hsDqI","executionInfo":{"status":"ok","timestamp":1707241494214,"user_tz":-330,"elapsed":488749,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}},"outputId":"65f1551a-13b9-4b42-e2d7-09860324005b"},"execution_count":38,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-37-ba0675ba863b>:29: RuntimeWarning: invalid value encountered in divide\n","  Bk = np.dot((np.eye(len(xk)) - np.outer(sk, yk) / np.dot(yk, sk)), np.dot(Bk, (np.eye(len(xk)) - np.outer(yk, sk) / np.dot(yk, sk)))) + np.outer(sk, sk) / np.dot(yk, sk)\n"]},{"output_type":"stream","name":"stdout","text":["for n = 1000\n","total iterations: 4 | minimizer: [nan nan nan nan nan] | minimum: nan | time taken: 1.1399421691894531\n","for n = 2500\n","total iterations: 4 | minimizer: [nan nan nan nan nan] | minimum: nan | time taken: 11.822689771652222\n","for n = 5000\n","total iterations: 4 | minimizer: [nan nan nan nan nan] | minimum: nan | time taken: 73.32806253433228\n","for n = 7500\n","total iterations: 4 | minimizer: [nan nan nan nan nan] | minimum: nan | time taken: 220.5196189880371\n","for n = 10000\n","total iterations: 4 | minimizer: [nan nan nan nan nan] | minimum: nan | time taken: 497.45763874053955\n"]}]},{"cell_type":"markdown","source":["**3. Implement Algorithm 2 for solving minx∈Rn f(x), Use backtracking line search with α0 = 0.9, ρ = 0.5, γ = 0.5.\n","Take the starting point to be $x_0 = (0, 0, ..., 0)$. Take n ∈ {1000, 2500, 5000, 7500, 10000}, find minimizer of the\n","objective function in each case and compute the time taken by the Newton’s method with backtracking line\n","search. Tabulate the time taken by Newton’s method for each n.**"],"metadata":{"id":"J_bOLFX3lnvt"}},{"cell_type":"code","source":["import numpy as np\n","import time\n","\n","def f(x):\n","    n = len(x)\n","    return sum([4 * (x[i]**2 - x[i+1])**2 + (x[i] - 1)**2 for i in range(n-1)])\n","\n","def gradient_f(x):\n","    n = len(x)\n","    grad = np.zeros(n)\n","    for i in range(n-1):\n","        grad[i] += 8 * (x[i]**2 - x[i+1]) * x[i] - 8 * (x[i]**2 - x[i+1])\n","        grad[i] += 2 * (x[i] - 1)\n","        grad[i+1] += -8 * (x[i]**2 - x[i+1])\n","    grad[-1] += 2 * (x[-1] - 1)\n","    return grad\n","\n","def hessian_f(x):\n","    n = len(x)\n","    hess = np.zeros((n, n))\n","    for i in range(n-1):\n","        hess[i][i] += 8 * (3 * x[i]**2 - 2 * x[i] * x[i+1] - x[i+1])\n","        hess[i][i+1] += -8 * x[i]\n","        hess[i+1][i] += -8 * x[i]\n","        hess[i+1][i+1] += 8 * x[i]\n","    hess[-1][-1] = 2\n","    return hess\n","\n","def backtracking_line_search(f, grad_f, x, delta_x, alpha=0.9, rho=0.5, gamma=0.5):\n","    eta = 1.0\n","    while f(x + eta * delta_x) > f(x) + alpha * eta * np.dot(grad_f(x), delta_x):\n","        eta *= rho\n","    return eta\n","\n","def newtons_method_with_line_search(f, grad_f, hess_f, x0, tau, alpha=0.9, rho=0.5, gamma=0.5, epsilon=1e-6):\n","    x = x0.copy()\n","    start_time = time.time()\n","    while np.linalg.norm(grad_f(x)) > tau:\n","        hess = hess_f(x)\n","        try:\n","            inv_hess = np.linalg.inv(hess + epsilon * np.eye(len(x)))\n","        except np.linalg.LinAlgError:\n","            # If the Hessian is singular, we use a different approach\n","            # or modify the Hessian matrix to make it non-singular\n","            break\n","        delta_x = -inv_hess.dot(grad_f(x))\n","        eta = backtracking_line_search(f, grad_f, x, delta_x, alpha, rho, gamma)\n","        x += eta * delta_x\n","    end_time = time.time()\n","    return x, end_time - start_time\n","\n","\n","# Main loop\n","n_values = [1000, 2500, 5000, 7500, 10000]\n","minimizers = []\n","times = []\n","\n","for n in n_values:\n","    x0 = np.zeros(n)\n","    tau = 1e-9\n","    minimizer, time_taken = newtons_method_with_line_search(f, gradient_f, hessian_f, x0, tau)\n","    minimizers.append(minimizer)\n","    times.append(time_taken)\n","\n","# Tabulate the results\n","print(\"n\\tMinimizer\\tTime (s)\")\n","for i, n in enumerate(n_values):\n","    print(f\"{n}\\t{minimizers[i]}\\t{times[i]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"GYot5yvbb-ok","executionInfo":{"status":"error","timestamp":1707244951563,"user_tz":-330,"elapsed":2203672,"user":{"displayName":"Rubul Gogoi","userId":"05033933680624392912"}},"outputId":"84ae652e-7616-4374-f343-3652698ce817"},"execution_count":40,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-bdbf40b49066>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mminimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewtons_method_with_line_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mminimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_taken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-bdbf40b49066>\u001b[0m in \u001b[0;36mnewtons_method_with_line_search\u001b[0;34m(f, grad_f, hess_f, x0, tau, alpha, rho, gamma, epsilon)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mdelta_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0minv_hess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktracking_line_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-bdbf40b49066>\u001b[0m in \u001b[0;36mbacktracking_line_search\u001b[0;34m(f, grad_f, x, delta_x, alpha, rho, gamma)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbacktracking_line_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0meta\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-bdbf40b49066>\u001b[0m in \u001b[0;36mgradient_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"D2ixuZDklzqY"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMGzeI8dQiCk/d030OhbPS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}